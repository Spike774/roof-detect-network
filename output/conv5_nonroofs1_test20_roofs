printer: <experiment_settings.PrintLogSave instance at 0x7f0b3ce8e9e0>
roofs_only: True
data_augmentation: True
scaler: StandardScaler
display_mistakes: True
non_roofs: 1
test_percent: 0.2
net: MyNeuralNet(X_tensor_type=None,
      batch_iterator_test=<FlipBatchIterator.CropOnlyBatchIterator object at 0x7f0b3cf08dd0>,
      batch_iterator_train=<FlipBatchIterator.FlipBatchIterator object at 0x7f0b3cf08e10>,
      custom_score=None, eval_size=0.2, input_shape=(None, 3, 32, 32),
      layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('conv1', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool1', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('conv2', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool2', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('conv3', <class..., <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],
      loss=None, max_epochs=250, more_params={},
      net_name='conv5_nonroofs1_test20_roofs',
      objective=<class 'lasagne.objectives.Objective'>,
      objective_loss_function=<function categorical_crossentropy at 0x7f0b579db8c0>,
      on_epoch_finished=[<experiment_settings.PrintLogSave instance at 0x7f0b3ce8e9e0>, <nolearn.lasagne.handlers.PrintLog instance at 0x7f0b3ce8ecf8>],
      on_training_finished=[],
      on_training_started=[<experiment_settings.SaveLayerInfo instance at 0x7f0b3ce8ec68>, <nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7f0b3ce8ed40>],
      output_nonlinearity=<function softmax at 0x7f0b46e65938>,
      output_num_units=3, preproc_scaler=None, regression=False,
      update=<function nesterov_momentum at 0x7f0b46c111b8>,
      update_learning_rate=0.01, update_momentum=0.9,
      use_label_encoder=False, verbose=1,
      y_tensor_type=TensorType(int32, vector))
preloaded: False

# Neural Network with 870551 learnable parameters
## Layer information

  #  name     size
---  -------  --------
  0  input    3x32x32
  1  conv1    32x30x30
  2  pool1    32x15x15
  3  conv2    64x14x14
  4  pool2    64x7x7
  5  conv3    128x6x6
  6  pool3    128x3x3
  7  hidden4  500
  8  hidden5  500
  9  output   3 

1	0.906614847985	0.730391845577	1.24127186451	0.7015625
2	0.6593370595	0.597376178511	1.10372171375	0.69765625
3	0.566145243437	0.505561195421	1.11983524164	0.77671875
4	0.485739554174	0.48586366491	0.999744556456	0.74109375
5	0.428189270706	0.414593443187	1.03279315615	0.81796875
6	0.405422227287	0.356184838646	1.13823549825	0.84015625
7	0.319857610784	0.344388207512	0.928770508999	0.84015625
8	0.30545847769	0.332744284349	0.917997669856	0.8540625
9	0.294829810286	0.300081862278	0.982497935892	0.88578125
10	0.283465290538	0.305612050507	0.92753309324	0.88578125
11	0.264455559753	0.313317472696	0.844049830601	0.8796875
12	0.258402060725	0.288271612546	0.896383998558	0.86796875
13	0.254796231253	0.353488884811	0.720804082395	0.89359375
14	0.249086080194	0.285670230562	0.871935727093	0.8875
15	0.253927342255	0.231925620273	1.09486542261	0.8996875
16	0.252463464383	0.22252714785	1.13452882861	0.90359375
17	0.226535364913	0.246312855237	0.919705813545	0.90359375
18	0.235396893411	0.247508468059	0.951066019101	0.9075
19	0.209059683456	0.252941694607	0.826513334548	0.90140625
20	0.220896704554	0.236845471993	0.93266171692	0.91140625
21	0.214660252437	0.198969212844	1.07886164583	0.90359375
22	0.223465240908	0.255332970899	0.875191480838	0.8975
23	0.204023801784	0.250155801652	0.815586927974	0.8975
24	0.191003610468	0.242144077407	0.788801495843	0.89359375
25	0.194630876481	0.199160717968	0.977255346671	0.9075
26	0.184157316204	0.242597679907	0.75910584254	0.90140625
27	0.186686778273	0.218414583249	0.854735867431	0.9153125
28	0.183864525868	0.18909533172	0.972337731429	0.90359375
29	0.201390087666	0.196011587433	1.02743970549	0.91140625
30	0.215707674182	0.198791921485	1.08509275714	0.90359375
31	0.16923249346	0.215359154624	0.785815182809	0.8975
32	0.17006762864	0.194100847928	0.876181791351	0.923125
33	0.163228756466	0.181734900986	0.898169562262	0.913125
34	0.175485239225	0.159256204867	1.10190519341	0.913125
35	0.170010398822	0.174429384984	0.974666045158	0.913125
36	0.152997936354	0.161200335979	0.949116733684	0.90703125
37	0.15304385279	0.154451421943	0.990886654618	0.93484375
38	0.173536611022	0.150468501453	1.1533085619	0.9209375
39	0.166131506269	0.245884767631	0.675647816129	0.89359375
40	0.150813087154	0.165547471123	0.910996019032	0.933125
41	0.143442550579	0.131327939326	1.09224702158	0.94484375
42	0.155964589082	0.141458831315	1.10254402382	0.93484375
43	0.12908810075	0.175930639266	0.733744282911	0.93484375
44	0.147546418882	0.138552421429	1.06491404018	0.94875
45	0.130543759776	0.174117562361	0.749744930987	0.9253125
46	0.153322019555	0.147550818335	1.03911331218	0.93484375
