printer: <experiment_settings.PrintLogSave instance at 0x7f28b73869e0>
roofs_only: True
data_augmentation: True
scaler: StandardScaler
display_mistakes: True
non_roofs: 1
test_percent: 0.2
net: MyNeuralNet(X_tensor_type=None,
      batch_iterator_test=<FlipBatchIterator.CropOnlyBatchIterator object at 0x7f28b7400dd0>,
      batch_iterator_train=<FlipBatchIterator.FlipBatchIterator object at 0x7f28b7400e10>,
      custom_score=None, eval_size=0.2, input_shape=(None, 3, 32, 32),
      layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('conv1', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool1', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('conv2', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool2', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],
      loss=None, max_epochs=250, more_params={},
      net_name='conv2_nonroofs1_test20_roofs',
      objective=<class 'lasagne.objectives.Objective'>,
      objective_loss_function=<function categorical_crossentropy at 0x7f28d1ed38c0>,
      on_epoch_finished=[<experiment_settings.PrintLogSave instance at 0x7f28b73869e0>, <nolearn.lasagne.handlers.PrintLog instance at 0x7f28b7386bd8>],
      on_training_finished=[],
      on_training_started=[<experiment_settings.SaveLayerInfo instance at 0x7f28b7386b48>, <nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7f28b7386c20>],
      output_nonlinearity=<function softmax at 0x7f28c135d938>,
      output_num_units=3, preproc_scaler=None, regression=False,
      update=<function nesterov_momentum at 0x7f28c11091b8>,
      update_learning_rate=0.01, update_momentum=0.9,
      use_label_encoder=False, verbose=1,
      y_tensor_type=TensorType(int32, vector))
preloaded: False

# Neural Network with 18563 learnable parameters
## Layer information

  #  name    size
---  ------  --------
  0  input   3x32x32
  1  conv1   32x30x30
  2  pool1   32x15x15
  3  conv2   64x14x14
  4  pool2   64x7x7
  5  output  3 

1	0.755130931765	0.61793470265	1.22202382958	0.74453125
2	0.540721902444	0.511573871385	1.05697716926	0.79796875
3	0.45911664221	0.445500167497	1.03056446598	0.86359375
4	0.443820098195	0.389931972864	1.13819878615	0.84578125
5	0.377204958702	0.374878188122	1.00620673769	0.8340625
6	0.326863501988	0.390838219471	0.836314069873	0.8284375
7	0.305584024925	0.390389436345	0.782767146021	0.82453125
8	0.31451952992	0.382412217551	0.822462033077	0.8640625
9	0.293328966553	0.370368599928	0.791991995567	0.86796875
10	0.277889710692	0.335342944074	0.828673200384	0.87578125
11	0.267432701244	0.306493553719	0.872555712835	0.87578125
12	0.261401472454	0.323696526936	0.807551056938	0.85578125
13	0.27877573134	0.295933153688	0.942022642161	0.8796875
14	0.2516248569	0.295685887896	0.850987034552	0.8775
15	0.238871039263	0.293114831456	0.814940131405	0.85578125
16	0.241477782421	0.275869657582	0.875332881976	0.86359375
17	0.22245218751	0.267425320006	0.831829190688	0.87359375
18	0.21887597715	0.260577973205	0.839963464518	0.87359375
19	0.208125517568	0.248085358731	0.83892704766	0.87359375
20	0.216425171175	0.2516776956	0.85992988238	0.89140625
21	0.205008051672	0.253211124246	0.809632879608	0.8775
22	0.223596928432	0.236338653321	0.946087003926	0.903125
23	0.201942997708	0.239386816795	0.843584456368	0.8875
24	0.213285982627	0.210802533224	1.0117809277	0.8953125
25	0.195945113678	0.225167246748	0.870220320706	0.8853125
26	0.201133470074	0.254961486285	0.78887785369	0.90140625
27	0.207130663904	0.220327034678	0.940105530883	0.8875
28	0.205730573124	0.233880285265	0.879640508777	0.89140625
29	0.189672863261	0.20353295333	0.931902476516	0.89140625
30	0.196606430605	0.212031848515	0.927249523982	0.8875
31	0.191096229604	0.20326914623	0.940114292542	0.90140625
32	0.178956440324	0.190614205632	0.938841046659	0.91140625
33	0.177670923146	0.193178253804	0.919725277806	0.9153125
34	0.179586463775	0.192236592192	0.934195002772	0.9153125
35	0.183228543165	0.198740153352	0.921950295774	0.8953125
36	0.177264897093	0.216101930477	0.820283727691	0.8875
37	0.192741661289	0.19633376132	0.98170411443	0.8875
38	0.181052292166	0.18931050654	0.956377411243	0.89140625
39	0.180272798659	0.184255468903	0.978385063587	0.9053125
40	0.171352461271	0.189359262937	0.904906676409	0.9053125
41	0.182057868506	0.208637574844	0.872603454298	0.90140625
42	0.180995184904	0.173058239566	1.04586285725	0.923125
43	0.171132929907	0.173517548542	0.986257190382	0.933125
44	0.179368274408	0.174037847652	1.03062797448	0.913125
45	0.16807258139	0.174235632487	0.964628067126	0.90921875
46	0.163049039044	0.179089485551	0.910433343096	0.913125
47	0.163884135292	0.191122178789	0.857483607242	0.913125
48	0.178171142863	0.163699095479	1.08840639798	0.93703125
49	0.170800794781	0.201331900526	0.848354355838	0.9053125
50	0.154870358472	0.182576161576	0.848250708828	0.8953125
51	0.157584411374	0.165133616603	0.954284261528	0.92921875
52	0.155316156295	0.189226353645	0.820795588479	0.90921875
53	0.161508619826	0.153390234542	1.05292635029	0.94484375
54	0.149899642411	0.152930920552	0.980178775288	0.94484375
55	0.141250361072	0.157161285355	0.898760536053	0.93875
56	0.146132017403	0.15284376568	0.956087523447	0.94875
57	0.142856005243	0.220695333603	0.647299618487	0.8875
58	0.171494601443	0.152219703466	1.12662551259	0.9409375
59	0.142623532988	0.145913008679	0.977455912111	0.95484375
60	0.132519429232	0.146350571628	0.905493075688	0.94875
61	0.141715986183	0.146741752599	0.965750944588	0.9409375
62	0.15595419575	0.195409498486	0.798089125443	0.9053125
63	0.154114176115	0.154572960124	0.997031925841	0.923125
64	0.133018344408	0.137740375705	0.965717885751	0.95484375
65	0.143169239291	0.143934889006	0.994680582867	0.95484375
66	0.143440029217	0.14519013593	0.987946104591	0.9509375
67	0.138994824701	0.144201364352	0.963893964015	0.95875
68	0.149752777007	0.138197541461	1.08361390097	0.9409375
69	0.137270133922	0.132462172444	1.03629686415	0.94875
70	0.126254368265	0.13407821682	0.941647131501	0.95265625
71	0.12204875938	0.13164636157	0.927095575782	0.94875
72	0.140289905278	0.134841529296	1.04040577121	0.95875
73	0.133791282619	0.137836782768	0.970650068381	0.94875
74	0.144609811725	0.132357581962	1.09256915683	0.94875
75	0.144131092773	0.12759080634	1.12963540954	0.9509375
76	0.122449994493	0.126502798704	0.967962730846	0.95265625
77	0.147791341464	0.138068605435	1.07041960045	0.95265625
78	0.116683058762	0.119196923439	0.978909986903	0.96265625
79	0.122932199214	0.120261704053	1.02220569867	0.9665625
80	0.107414144105	0.118822661561	0.903987023133	0.96265625
81	0.11820113818	0.122448397564	0.965313883487	0.95875
82	0.125404619375	0.128707783274	0.974335942896	0.95875
83	0.116245249049	0.130677180849	0.889560428942	0.95875
84	0.137988647915	0.136352447368	1.01199978863	0.95265625
85	0.109879352423	0.121698705692	0.902880205655	0.9665625
86	0.108529863096	0.12395236902	0.875577158825	0.95875
87	0.120020181036	0.122083490751	0.983099191359	0.94875
88	0.123484019146	0.119152782947	1.03635027308	0.96265625
