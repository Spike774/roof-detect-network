printer: <experiment_settings.PrintLogSave instance at 0x7fd9c67069e0>
roofs_only: True
data_augmentation: True
scaler: StandardScaler
display_mistakes: True
non_roofs: 1
test_percent: 0.2
net: MyNeuralNet(X_tensor_type=None,
      batch_iterator_test=<FlipBatchIterator.CropOnlyBatchIterator object at 0x7fd9c677ee10>,
      batch_iterator_train=<FlipBatchIterator.FlipBatchIterator object at 0x7fd9c677ee50>,
      custom_score=None, eval_size=0.2, input_shape=(None, 3, 32, 32),
      layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('conv1', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool1', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],
      loss=None, max_epochs=250, more_params={},
      net_name='conv1_nonroofs1_test20_roofs',
      objective=<class 'lasagne.objectives.Objective'>,
      objective_loss_function=<function categorical_crossentropy at 0x7fd9e12548c0>,
      on_epoch_finished=[<experiment_settings.PrintLogSave instance at 0x7fd9c67069e0>, <nolearn.lasagne.handlers.PrintLog instance at 0x7fd9c6706b90>],
      on_training_finished=[],
      on_training_started=[<experiment_settings.SaveLayerInfo instance at 0x7fd9c6706ab8>, <nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7fd9c6706bd8>],
      output_nonlinearity=<function softmax at 0x7fd9d065e938>,
      output_num_units=3, preproc_scaler=None, regression=False,
      update=<function nesterov_momentum at 0x7fd9d04881b8>,
      update_learning_rate=0.01, update_momentum=0.9,
      use_label_encoder=False, verbose=1,
      y_tensor_type=TensorType(int32, vector))
plot_loss: True
preloaded: False

# Neural Network with 22499 learnable parameters
## Layer information

  #  name    size
---  ------  --------
  0  input   3x32x32
  1  conv1   32x30x30
  2  pool1   32x15x15
  3  output  3 

1	0.63456357462	0.48477187445	1.30899420545	0.800625
2	0.401115501041	0.4620653656	0.868092549028	0.7928125
3	0.38250293433	0.41646632266	0.918448656033	0.80671875
4	0.344660744908	0.402143342866	0.857059431724	0.800625
5	0.334478401779	0.377789838016	0.885355740471	0.8284375
6	0.294767428003	0.350417685318	0.84118878799	0.8484375
7	0.272227141046	0.34212034527	0.795705794202	0.871875
8	0.259610420775	0.32785666367	0.791841220702	0.861875
9	0.25975328867	0.282202737145	0.920449217812	0.87359375
10	0.250987999636	0.288751864879	0.869216895765	0.87359375
11	0.237226907677	0.280396645517	0.846040462571	0.8596875
12	0.227462970799	0.283645099362	0.801928083054	0.8675
13	0.218849865116	0.259553911582	0.843176909885	0.87921875
14	0.219557605518	0.25564346841	0.858843008522	0.8753125
15	0.212485579123	0.241422278381	0.880140724991	0.87140625
16	0.208809426393	0.304874211156	0.684903539732	0.861875
17	0.241580578607	0.236585418354	1.02111355927	0.8753125
18	0.191970779154	0.215964459078	0.888899867937	0.8853125
19	0.194892073385	0.203407754152	0.958134925571	0.92703125
20	0.20623439704	0.202489155243	1.01849601176	0.90921875
21	0.194846723887	0.201493375537	0.967013051261	0.89921875
